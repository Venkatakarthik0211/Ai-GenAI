# Flow Diagrams - MLOps Pipeline Automation
## Level 1, Level 2, Level 3 Design

## Document Information
- **Version**: 1.0
- **Date**: 2025-11-30
- **Purpose**: Multi-level flow diagrams from high-level overview to implementation details

---

## Table of Contents
1. [Level 1 Design (1LD) - High-Level Overview](#1-level-1-design-1ld---high-level-overview)
2. [Level 2 Design (2LD) - Detailed Component Interactions](#2-level-2-design-2ld---detailed-component-interactions)
3. [Level 3 Design (3LD) - Implementation Details](#3-level-3-design-3ld---implementation-details)

---

## 1. Level 1 Design (1LD) - High-Level Overview

### 1.1 Complete MLOps Pipeline Flow (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LEVEL 1: HIGH-LEVEL FLOW                           â”‚
â”‚                  (Conceptual View)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            â•”â•â•â•â•â•â•â•â•â•â•â•â•—
                            â•‘   START   â•‘
                            â•šâ•â•â•â•â•â•¦â•â•â•â•â•â•
                                  â•‘
                                  â–¼
                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                    â•‘   1. DATA PREPARATION      â•‘
                    â•‘   â€¢ Load data              â•‘
                    â•‘   ğŸ‘¤ Human review          â•‘ â—„â”€â”€â”€ NEW: Human-in-the-Loop
                    â•‘   â€¢ Clean & preprocess     â•‘
                    â•‘   â€¢ Split train/test       â•‘
                    â•‘   â€¢ Select features        â•‘
                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                  â•‘
                                  â–¼
                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                    â•‘   2. AI-POWERED TRAINING   â•‘
                    â•‘   ğŸ¤– Agent: Select algos   â•‘
                    â•‘   â€¢ Train multiple models  â•‘
                    â•‘   â€¢ Hyperparameter tuning  â•‘
                    â•‘   ğŸ¤– Agent: Select best    â•‘
                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                  â•‘
                                  â–¼
                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                    â•‘   3. MODEL EVALUATION      â•‘
                    â•‘   â€¢ Test set metrics       â•‘
                    â•‘   â€¢ Performance analysis   â•‘
                    â•‘   â€¢ Log to MLflow          â•‘
                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                  â•‘
                                  â–¼
                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                    â•‘   4. MONITORING            â•‘
                    â•‘   â€¢ Drift detection        â•‘
                    â•‘   â€¢ Performance tracking   â•‘
                    â•‘   ğŸ¤– Agent: Retrain?       â•‘
                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                  â•‘
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚                     â”‚
                 (Retrain Needed)      (Model Good)
                       â”‚                     â”‚
                       â–¼                     â–¼
            â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
            â•‘ 5a. RETRIGGER   â•‘   â•‘ 5b. DEPLOY & REPORT â•‘
            â•‘ â€¢ Loop to START â•‘   â•‘ â€¢ Register model    â•‘
            â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â•‘ â€¢ Generate report   â•‘
                                  â•šâ•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•
                                             â•‘
                                             â–¼
                                      â•”â•â•â•â•â•â•â•â•â•â•â•â•—
                                      â•‘    END    â•‘
                                      â•šâ•â•â•â•â•â•â•â•â•â•â•â•

Key Stakeholders:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Data Scientist: Configures & monitors pipeline
â€¢ ML Engineer: Manages deployment
â€¢ Business User: Consumes insights
```

### 1.2 User Interaction Flow (High-Level)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             LEVEL 1: USER INTERACTION FLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     User                 Streamlit UI            MLOps System
      â”‚                        â”‚                        â”‚
      â”‚  1. Open Dashboard     â”‚                        â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                        â”‚
      â”‚                        â”‚                        â”‚
      â”‚  2. Upload Data        â”‚                        â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                        â”‚
      â”‚                        â”‚                        â”‚
      â”‚  3. Configure Pipeline â”‚                        â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                        â”‚
      â”‚                        â”‚                        â”‚
      â”‚  4. Start Pipeline     â”‚  5. Execute Pipeline   â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚                        â”‚                        â”‚
      â”‚  6. Monitor Progress   â”‚  7. Real-time Updates  â”‚
      â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚                        â”‚                        â”‚
      â”‚  8. View Results       â”‚  9. Fetch Results      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚                        â”‚  10. Return Results    â”‚
      â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚                        â”‚                        â”‚
      â”‚  11. Review & Deploy   â”‚  12. Deploy Model      â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚                        â”‚                        â”‚
```

---

## 2. Level 2 Design (2LD) - Detailed Component Interactions

### 2.1 Complete Pipeline with Component Details

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  LEVEL 2: DETAILED COMPONENT FLOW                   â”‚
â”‚                  (Component Interactions)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  START  â”‚
                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PHASE 1: DATA PREPARATION                             â”‚
    â”‚                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
    â”‚  â”‚ DataLoader       â”‚                                  â”‚
    â”‚  â”‚ â€¢ load_csv()     â”‚â”€â”€â”€â”€â”€> raw_data: DataFrame       â”‚
    â”‚  â”‚ â€¢ validate()     â”‚                                  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚ AlgorithmCategoryPredictor   â”‚ â—„â”€â”€â”€ NEW: Agent 1A   â”‚
    â”‚  â”‚ (ğŸ¤– Agent 1A - Bedrock)      â”‚                      â”‚
    â”‚  â”‚ â€¢ analyze_data_profile()     â”‚                      â”‚
    â”‚  â”‚ â€¢ predict_algorithm_category()â”‚                     â”‚
    â”‚  â”‚   (tree_models, linear_modelsâ”‚                      â”‚
    â”‚  â”‚    neural_networks, etc.)    â”‚                      â”‚
    â”‚  â”‚ â€¢ determine_preprocessing_   â”‚                      â”‚
    â”‚  â”‚   priorities()               â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚           â”‚ algorithm_category,  â”‚                      â”‚
    â”‚           â”‚ confidence, requirements                    â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚ PreprocessingQuestionGen     â”‚ â—„â”€â”€â”€ NEW: Agent 1B   â”‚
    â”‚  â”‚ (ğŸ¤– Agent 1B - Bedrock)      â”‚                      â”‚
    â”‚  â”‚ â€¢ build_algorithm_context()  â”‚                      â”‚
    â”‚  â”‚ â€¢ generate_dynamic_questions()â”‚                     â”‚
    â”‚  â”‚   (4-20 questions based on   â”‚                      â”‚
    â”‚  â”‚    algorithm & data needs)   â”‚                      â”‚
    â”‚  â”‚ â€¢ generate_technique_options()â”‚                     â”‚
    â”‚  â”‚   (ranked by algorithm       â”‚                      â”‚
    â”‚  â”‚    suitability)              â”‚                      â”‚
    â”‚  â”‚ â€¢ store_review_session()     â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚ [LangGraph Interruption]     â”‚ â—„â”€â”€â”€ Pipeline Pauses â”‚
    â”‚  â”‚ Status: "awaiting_review"    â”‚                      â”‚
    â”‚  â”‚ Waiting for human approval   â”‚                      â”‚
    â”‚  â”‚ + algorithm_category         â”‚                      â”‚
    â”‚  â”‚ + question_count (4-20)      â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚ ğŸ‘¤ Human Decision            â”‚                      â”‚
    â”‚  â”‚ Algorithm-Aware UI:          â”‚                      â”‚
    â”‚  â”‚ â€¢ Algorithm category banner  â”‚                      â”‚
    â”‚  â”‚ â€¢ Tabbed interface (4 steps) â”‚                      â”‚
    â”‚  â”‚ â€¢ Review 4-20 questions      â”‚                      â”‚
    â”‚  â”‚ â€¢ Select techniques from     â”‚                      â”‚
    â”‚  â”‚   ranked options             â”‚                      â”‚
    â”‚  â”‚ â€¢ Adjust technique parametersâ”‚                      â”‚
    â”‚  â”‚ â€¢ Approve / Reject           â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼ (If Approved)                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    â”‚  â”‚ Technique-Based Preprocessor â”‚â”€>â”‚  MLflow Logger  â”‚â”‚
    â”‚  â”‚ â€¢ clean_data_node()          â”‚  â”‚  log_params()   â”‚â”‚
    â”‚  â”‚   reads technique from       â”‚  â”‚  log_metrics()  â”‚â”‚
    â”‚  â”‚   review_answers             â”‚  â”‚  log_artifacts()â”‚â”‚
    â”‚  â”‚   executes user-selected     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    â”‚  â”‚   technique (e.g., "none",   â”‚                     â”‚
    â”‚  â”‚   "iqr_method", etc.)        â”‚                     â”‚
    â”‚  â”‚ â€¢ handle_missing_node()      â”‚                     â”‚
    â”‚  â”‚   executes selected techniqueâ”‚                     â”‚
    â”‚  â”‚   with parameters            â”‚                     â”‚
    â”‚  â”‚ â€¢ encode_features_node()     â”‚                     â”‚
    â”‚  â”‚   executes selected encoding â”‚                     â”‚
    â”‚  â”‚ â€¢ scale_features_node()      â”‚                     â”‚
    â”‚  â”‚   executes selected scaler   â”‚                     â”‚
    â”‚  â”‚   (or skips if "none")       â”‚                     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
    â”‚  â”‚ TrainTestSplitterâ”‚â”€â”€â”€â”€â”€> X_train, X_test,          â”‚
    â”‚  â”‚ â€¢ split(0.8/0.2) â”‚       y_train, y_test           â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚ FeatureSelector  â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  MLflow Logger  â”‚     â”‚
    â”‚  â”‚ â€¢ calculate_importance()â”‚   â”‚  log_artifact() â”‚     â”‚
    â”‚  â”‚ â€¢ select_top_k() â”‚         â”‚  (importance plot)â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚           â”‚                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PHASE 2: AI-POWERED ALGORITHM SELECTION               â”‚
    â”‚                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
    â”‚  â”‚ ContextBuilder   â”‚                                  â”‚
    â”‚  â”‚ â€¢ extract_stats()â”‚                                  â”‚
    â”‚  â”‚ â€¢ format_context()â”€â”€â”€â”€> context_json               â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚ AlgorithmSelectionAgent (ğŸ¤–) â”‚                      â”‚
    â”‚  â”‚ â€¢ prepare_prompt()           â”‚                      â”‚
    â”‚  â”‚ â€¢ invoke_bedrock()           â”‚                      â”‚
    â”‚  â”‚ â€¢ parse_decision()           â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  Decision: {                                           â”‚
    â”‚    "selected": ["RF", "GB", "LR"],                     â”‚
    â”‚    "skip": ["SVM", "KNN"],                             â”‚
    â”‚    "param_suggestions": {...}                          â”‚
    â”‚  }                                                      â”‚
    â”‚           â”‚                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PHASE 3: PARALLEL MODEL TRAINING                      â”‚
    â”‚                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ RF Trainer  â”‚  â”‚ GB Trainer  â”‚  â”‚ LR Trainer  â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚         â”‚                â”‚                â”‚            â”‚
    â”‚         â–¼                â–¼                â–¼            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚     HyperparameterTuner                      â”‚     â”‚
    â”‚  â”‚     (GridSearchCV)                           â”‚     â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚
    â”‚  â”‚  â”‚  For each param combination:           â”‚  â”‚     â”‚
    â”‚  â”‚  â”‚  1. Create estimator                   â”‚  â”‚     â”‚
    â”‚  â”‚  â”‚  2. Cross-validate (5-fold)            â”‚  â”‚     â”‚
    â”‚  â”‚  â”‚  3. Calculate CV score                 â”‚  â”‚     â”‚
    â”‚  â”‚  â”‚  4. Track best params                  â”‚  â”‚     â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                         â”‚                              â”‚
    â”‚                         â–¼                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚     MLflow Logger (Child Runs)               â”‚     â”‚
    â”‚  â”‚  â€¢ Log best_params per algorithm             â”‚     â”‚
    â”‚  â”‚  â€¢ Log CV scores                             â”‚     â”‚
    â”‚  â”‚  â€¢ Log trained models                        â”‚     â”‚
    â”‚  â”‚  â€¢ Log training time                         â”‚     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                         â”‚                              â”‚
    â”‚                         â–¼                              â”‚
    â”‚  algorithm_results: {                                 â”‚
    â”‚    "random_forest": AlgorithmResult(...),             â”‚
    â”‚    "gradient_boosting": AlgorithmResult(...),         â”‚
    â”‚    "logistic_regression": AlgorithmResult(...)        â”‚
    â”‚  }                                                     â”‚
    â”‚                         â”‚                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PHASE 4: AI-POWERED MODEL SELECTION                   â”‚
    â”‚                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚ ModelSelectionAgent (ğŸ¤–)     â”‚                      â”‚
    â”‚  â”‚ â€¢ compare_models()           â”‚                      â”‚
    â”‚  â”‚ â€¢ analyze_tradeoffs()        â”‚                      â”‚
    â”‚  â”‚ â€¢ invoke_bedrock()           â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  Decision: {                                           â”‚
    â”‚    "selected_model": "gradient_boosting",              â”‚
    â”‚    "reasoning": "Best accuracy...",                    â”‚
    â”‚    "confidence": 0.95                                  â”‚
    â”‚  }                                                      â”‚
    â”‚           â”‚                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PHASE 5: MODEL EVALUATION                             â”‚
    â”‚                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚ ModelEvaluator   â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  MLflow Logger  â”‚     â”‚
    â”‚  â”‚ â€¢ predict(X_test)â”‚         â”‚  log_metrics()  â”‚     â”‚
    â”‚  â”‚ â€¢ calc_metrics() â”‚         â”‚  log_artifacts()â”‚     â”‚
    â”‚  â”‚ â€¢ confusion_matrix()â”‚       â”‚  (plots)        â”‚     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚                                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PHASE 6: MONITORING                                   â”‚
    â”‚                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚ DriftDetector    â”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  MLflow Logger  â”‚     â”‚
    â”‚  â”‚ â€¢ ks_test()      â”‚         â”‚  log_metrics()  â”‚     â”‚
    â”‚  â”‚ â€¢ chi2_test()    â”‚         â”‚  (drift_score)  â”‚     â”‚
    â”‚  â”‚ â€¢ calculate_psi()â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
    â”‚  â”‚PerformanceMonitorâ”‚â”€â”€â”€â”€â”€â”€â”€â”€>â”‚  MLflow Logger  â”‚     â”‚
    â”‚  â”‚ â€¢ compare()      â”‚         â”‚  log_metrics()  â”‚     â”‚
    â”‚  â”‚ â€¢ check_threshold()â”‚       â”‚  (perf_drop)    â”‚     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
    â”‚           â”‚                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PHASE 7: AI-POWERED RETRAINING DECISION               â”‚
    â”‚                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
    â”‚  â”‚ RetrainingDecisionAgent (ğŸ¤–) â”‚                      â”‚
    â”‚  â”‚ â€¢ analyze_drift()            â”‚                      â”‚
    â”‚  â”‚ â€¢ analyze_performance()      â”‚                      â”‚
    â”‚  â”‚ â€¢ invoke_bedrock()           â”‚                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
    â”‚           â”‚                                             â”‚
    â”‚           â–¼                                             â”‚
    â”‚  Decision: {                                           â”‚
    â”‚    "retrain": true/false,                              â”‚
    â”‚    "reasoning": "...",                                 â”‚
    â”‚    "urgency": "high"                                   â”‚
    â”‚  }                                                      â”‚
    â”‚           â”‚                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚
  (retrain=true)    (retrain=false)
       â”‚                 â”‚
       â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RETRIGGER   â”‚   â”‚ FINALIZATION         â”‚
â”‚ â€¢ loop back â”‚   â”‚ â€¢ register_model()   â”‚
â”‚ to START    â”‚   â”‚ â€¢ generate_report()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â€¢ end_mlflow_run()   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   END    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Data Flow Through System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             LEVEL 2: DATA FLOW THROUGH SYSTEM                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Input                      Processing                    Output
  â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”€â”€â”€â”€â”€â”€

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚train.csvâ”‚                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚10K rows â”‚â”€â”€â”€â”€â”€â”€> DataLoader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ raw_data:    â”‚
â”‚25 cols  â”‚                                         â”‚ DataFrame    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â”‚ (10000, 25)  â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â”‚
                                                           â–¼
                                           DataPreprocessor
                                           â€¢ Handle missing
                                           â€¢ Encode categorical
                                           â€¢ Scale numerical
                                                           â”‚
                                                           â–¼
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚ cleaned_data:â”‚
                                                    â”‚ DataFrame    â”‚
                                                    â”‚ (9800, 35)   â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                           â”‚
                                                           â–¼
                                            TrainTestSplitter
                                            split(test_size=0.2)
                                                           â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
                          â”‚                                     â”‚
                          â–¼                                     â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ X_train:    â”‚                      â”‚ X_test:     â”‚
                   â”‚ (7840, 35)  â”‚                      â”‚ (1960, 35)  â”‚
                   â”‚ y_train:    â”‚                      â”‚ y_test:     â”‚
                   â”‚ (7840,)     â”‚                      â”‚ (1960,)     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                                     â”‚
                          â–¼                                     â–¼
                    FeatureSelector                       (pass through)
                    select_top_k(15)
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ X_train:    â”‚                      â”‚ X_test:     â”‚
                   â”‚ (7840, 15)  â”‚                      â”‚ (1960, 15)  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                                     â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                                  ğŸ¤– Agent 1
                              Algorithm Selection
                                         â”‚
                                         â–¼
                              selected_algorithms:
                              ["RF", "GB", "LR"]
                                         â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚                      â”‚                      â”‚
                  â–¼                      â–¼                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ RF Training  â”‚      â”‚ GB Training  â”‚      â”‚ LR Training  â”‚
           â”‚ GridSearchCV â”‚      â”‚ GridSearchCV â”‚      â”‚ GridSearchCV â”‚
           â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
           â”‚ best_model   â”‚      â”‚ best_model   â”‚      â”‚ best_model   â”‚
           â”‚ cv_mean=0.87 â”‚      â”‚ cv_mean=0.89 â”‚      â”‚ cv_mean=0.84 â”‚
           â”‚ test_acc=0.88â”‚      â”‚ test_acc=0.90â”‚      â”‚ test_acc=0.85â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                      â”‚                      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                              algorithm_results:
                              {RF: {...}, GB: {...}, LR: {...}}
                                         â”‚
                                         â–¼
                                  ğŸ¤– Agent 2
                               Model Selection
                                         â”‚
                                         â–¼
                               best_model_name: "GB"
                               best_model: <GBClassifier>
                                         â”‚
                                         â–¼
                                  ModelEvaluator
                                  predict(X_test)
                                         â”‚
                                         â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ predictions:         â”‚
                              â”‚ [0,1,1,0,...] (1960) â”‚
                              â”‚                      â”‚
                              â”‚ metrics:             â”‚
                              â”‚  accuracy: 0.90      â”‚
                              â”‚  f1: 0.89            â”‚
                              â”‚  precision: 0.91     â”‚
                              â”‚  recall: 0.88        â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                                  DriftDetector
                              compare(train, test)
                                         â”‚
                                         â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ drift_report:        â”‚
                              â”‚  detected: True      â”‚
                              â”‚  score: 0.15         â”‚
                              â”‚  features: [f2, f5]  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                              PerformanceMonitor
                          compare(current, baseline)
                                         â”‚
                                         â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ perf_report:         â”‚
                              â”‚  drop: 0.05 (5.56%)  â”‚
                              â”‚  threshold_exceeded  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                                  ğŸ¤– Agent 3
                             Retraining Decision
                                         â”‚
                                         â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ retrain_decision:    â”‚
                              â”‚  retrain: True       â”‚
                              â”‚  reasoning: "..."    â”‚
                              â”‚  urgency: "high"     â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚                               â”‚
                 (retrain=True)                  (retrain=False)
                         â”‚                               â”‚
                         â–¼                               â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Loop to     â”‚              â”‚ ModelRegistry    â”‚
                  â”‚ START       â”‚              â”‚ register_model() â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                  â”‚
                                               â”‚ model_version: 2 â”‚
                                               â”‚ stage: Productionâ”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Level 3 Design (3LD) - Implementation Details

### 3.1 Data Preprocessing Implementation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LEVEL 3: DATA PREPROCESSING DETAILS                       â”‚
â”‚           (Implementation Specifics)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

class DataPreprocessor:
    def fit_transform(df: pd.DataFrame) -> pd.DataFrame:
        â”‚
        â”œâ”€> 1. MISSING VALUE ANALYSIS
        â”‚   â”œâ”€> For each column:
        â”‚   â”‚   â”œâ”€> Calculate missing_pct = null_count / total_rows
        â”‚   â”‚   â”œâ”€> if missing_pct > 0.7:
        â”‚   â”‚   â”‚   â””â”€> Drop column
        â”‚   â”‚   â”œâ”€> elif missing_pct > 0:
        â”‚   â”‚   â”‚   â”œâ”€> if numeric:
        â”‚   â”‚   â”‚   â”‚   â””â”€> Impute with mean or median
        â”‚   â”‚   â”‚   â””â”€> if categorical:
        â”‚   â”‚   â”‚       â””â”€> Impute with mode
        â”‚   â”‚   â””â”€> Log imputation details
        â”‚   â”‚
        â”‚   â””â”€> Return: df_imputed, imputation_metadata
        â”‚
        â”œâ”€> 2. OUTLIER DETECTION
        â”‚   â”œâ”€> For numeric columns:
        â”‚   â”‚   â”œâ”€> Calculate IQR:
        â”‚   â”‚   â”‚   Q1 = 25th percentile
        â”‚   â”‚   â”‚   Q3 = 75th percentile
        â”‚   â”‚   â”‚   IQR = Q3 - Q1
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> Define bounds:
        â”‚   â”‚   â”‚   lower_bound = Q1 - 1.5 * IQR
        â”‚   â”‚   â”‚   upper_bound = Q3 + 1.5 * IQR
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> Identify outliers:
        â”‚   â”‚   â”‚   outliers = (df[col] < lower_bound) | (df[col] > upper_bound)
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€> Remove or cap outliers
        â”‚   â”‚
        â”‚   â””â”€> Return: df_no_outliers, outlier_count
        â”‚
        â”œâ”€> 3. CATEGORICAL ENCODING
        â”‚   â”œâ”€> For each categorical column:
        â”‚   â”‚   â”œâ”€> Check cardinality:
        â”‚   â”‚   â”‚   unique_count = df[col].nunique()
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> if unique_count == 2:
        â”‚   â”‚   â”‚   â””â”€> Label encoding (0, 1)
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> elif unique_count < 10:
        â”‚   â”‚   â”‚   â””â”€> One-hot encoding
        â”‚   â”‚   â”‚       new_cols = pd.get_dummies(df[col], prefix=col)
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€> else:
        â”‚   â”‚       â””â”€> Target encoding or frequency encoding
        â”‚   â”‚
        â”‚   â””â”€> Return: df_encoded, encoding_mappings
        â”‚
        â”œâ”€> 4. FEATURE SCALING
        â”‚   â”œâ”€> Separate numeric columns
        â”‚   â”‚
        â”‚   â”œâ”€> Initialize StandardScaler:
        â”‚   â”‚   scaler = StandardScaler()
        â”‚   â”‚
        â”‚   â”œâ”€> Fit and transform:
        â”‚   â”‚   scaled_features = scaler.fit_transform(df[numeric_cols])
        â”‚   â”‚   df[numeric_cols] = scaled_features
        â”‚   â”‚
        â”‚   â”œâ”€> Store scaler:
        â”‚   â”‚   self.scaler = scaler
        â”‚   â”‚
        â”‚   â””â”€> Return: df_scaled
        â”‚
        â”œâ”€> 5. DATA VALIDATION
        â”‚   â”œâ”€> Check for:
        â”‚   â”‚   â”œâ”€> Remaining nulls: assert df.isnull().sum().sum() == 0
        â”‚   â”‚   â”œâ”€> Infinite values: assert not np.isinf(df.select_dtypes(include=[np.number])).any().any()
        â”‚   â”‚   â”œâ”€> Duplicate rows: duplicates = df.duplicated().sum()
        â”‚   â”‚   â””â”€> Feature types: all numeric or encoded
        â”‚   â”‚
        â”‚   â””â”€> Return: validation_report
        â”‚
        â””â”€> 6. MLFLOW LOGGING
            â”œâ”€> mlflow.log_params({
            â”‚       "imputation_method": self.imputation_method,
            â”‚       "scaling_method": "StandardScaler",
            â”‚       "encoding_method": "mixed",
            â”‚       "outliers_removed": outlier_count
            â”‚   })
            â”‚
            â”œâ”€> mlflow.log_metrics({
            â”‚       "original_rows": original_rows,
            â”‚       "final_rows": final_rows,
            â”‚       "original_cols": original_cols,
            â”‚       "final_cols": final_cols,
            â”‚       "dropped_cols": dropped_cols
            â”‚   })
            â”‚
            â””â”€> mlflow.log_artifact("preprocessing_report.json")

        Return: df_preprocessed, metadata
```

### 3.2 GridSearchCV Training Implementation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LEVEL 3: GRIDSEARCHCV TRAINING DETAILS                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

class RandomForestTrainer(BaseAlgorithmTrainer):
    def train(X_train, y_train, X_test, y_test) -> AlgorithmResult:
        â”‚
        â”œâ”€> 1. MLFLOW CHILD RUN SETUP
        â”‚   â”œâ”€> with mlflow.start_run(nested=True, run_name="random_forest"):
        â”‚   â”‚   â”œâ”€> child_run_id = mlflow.active_run().info.run_id
        â”‚   â”‚   â””â”€> Log start time
        â”‚   â”‚
        â”‚   â””â”€> Store: self.run_id = child_run_id
        â”‚
        â”œâ”€> 2. PARAMETER GRID DEFINITION
        â”‚   â”œâ”€> Get suggestions from Agent 1 (if available)
        â”‚   â”‚   param_grid_suggested = agent_decision.get("param_grid_rf")
        â”‚   â”‚
        â”‚   â”œâ”€> Fallback to default grid:
        â”‚   â”‚   param_grid = {
        â”‚   â”‚       'n_estimators': [100, 200, 300],
        â”‚   â”‚       'max_depth': [10, 20, 30, None],
        â”‚   â”‚       'min_samples_split': [2, 5, 10],
        â”‚   â”‚       'min_samples_leaf': [1, 2, 4],
        â”‚   â”‚       'max_features': ['sqrt', 'log2'],
        â”‚   â”‚       'bootstrap': [True, False]
        â”‚   â”‚   }
        â”‚   â”‚
        â”‚   â””â”€> Total combinations = 3 Ã— 4 Ã— 3 Ã— 3 Ã— 2 Ã— 2 = 432
        â”‚
        â”œâ”€> 3. GRIDSEARCHCV INITIALIZATION
        â”‚   â”œâ”€> estimator = RandomForestClassifier(
        â”‚   â”‚       random_state=42,
        â”‚   â”‚       n_jobs=1  # n_jobs in GridSearchCV, not estimator
        â”‚   â”‚   )
        â”‚   â”‚
        â”‚   â”œâ”€> cv_strategy = StratifiedKFold(
        â”‚   â”‚       n_splits=5,
        â”‚   â”‚       shuffle=True,
        â”‚   â”‚       random_state=42
        â”‚   â”‚   )
        â”‚   â”‚
        â”‚   â””â”€> grid_search = GridSearchCV(
        â”‚           estimator=estimator,
        â”‚           param_grid=param_grid,
        â”‚           cv=cv_strategy,
        â”‚           scoring='f1_weighted',
        â”‚           n_jobs=-1,  # Use all CPU cores
        â”‚           verbose=2,
        â”‚           return_train_score=True
        â”‚       )
        â”‚
        â”œâ”€> 4. GRID SEARCH EXECUTION
        â”‚   â”œâ”€> start_time = time.time()
        â”‚   â”‚
        â”‚   â”œâ”€> grid_search.fit(X_train, y_train)
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€> Internal process:
        â”‚   â”‚       For each param_combination in param_grid:
        â”‚   â”‚           â”œâ”€> Create estimator with params
        â”‚   â”‚           â”‚
        â”‚   â”‚           â”œâ”€> For each fold in cv (5 folds):
        â”‚   â”‚           â”‚   â”œâ”€> Split data into train_fold, val_fold
        â”‚   â”‚           â”‚   â”œâ”€> estimator.fit(train_fold)
        â”‚   â”‚           â”‚   â”œâ”€> predictions = estimator.predict(val_fold)
        â”‚   â”‚           â”‚   â”œâ”€> score = f1_weighted(val_fold, predictions)
        â”‚   â”‚           â”‚   â””â”€> Store fold_score
        â”‚   â”‚           â”‚
        â”‚   â”‚           â”œâ”€> Calculate mean_cv_score, std_cv_score
        â”‚   â”‚           â”‚
        â”‚   â”‚           â””â”€> if mean_cv_score > best_score:
        â”‚   â”‚               best_score = mean_cv_score
        â”‚   â”‚               best_params = param_combination
        â”‚   â”‚
        â”‚   â”œâ”€> training_time = time.time() - start_time
        â”‚   â”‚
        â”‚   â””â”€> Extract best estimator:
        â”‚       best_model = grid_search.best_estimator_
        â”‚
        â”œâ”€> 5. TEST SET EVALUATION
        â”‚   â”œâ”€> y_pred = best_model.predict(X_test)
        â”‚   â”‚
        â”‚   â”œâ”€> Calculate metrics:
        â”‚   â”‚   â”œâ”€> test_accuracy = accuracy_score(y_test, y_pred)
        â”‚   â”‚   â”œâ”€> test_f1 = f1_score(y_test, y_pred, average='weighted')
        â”‚   â”‚   â”œâ”€> test_precision = precision_score(y_test, y_pred, average='weighted')
        â”‚   â”‚   â””â”€> test_recall = recall_score(y_test, y_pred, average='weighted')
        â”‚   â”‚
        â”‚   â””â”€> Generate confusion matrix:
        â”‚       cm = confusion_matrix(y_test, y_pred)
        â”‚
        â”œâ”€> 6. CV RESULTS EXTRACTION
        â”‚   â”œâ”€> cv_results_df = pd.DataFrame(grid_search.cv_results_)
        â”‚   â”‚
        â”‚   â”œâ”€> Extract key info:
        â”‚   â”‚   â”œâ”€> cv_scores = grid_search.cv_results_['split0_test_score'],
        â”‚   â”‚   â”‚                 ...
        â”‚   â”‚   â”‚                 grid_search.cv_results_['split4_test_score']
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> cv_mean = grid_search.best_score_
        â”‚   â”‚   â””â”€> cv_std = grid_search.cv_results_['std_test_score'][best_index]
        â”‚   â”‚
        â”‚   â””â”€> Store: cv_results.json
        â”‚
        â”œâ”€> 7. MLFLOW LOGGING
        â”‚   â”œâ”€> Log parameters:
        â”‚   â”‚   mlflow.log_params(grid_search.best_params_)
        â”‚   â”‚   mlflow.log_param("cv_folds", 5)
        â”‚   â”‚   mlflow.log_param("scoring", "f1_weighted")
        â”‚   â”‚
        â”‚   â”œâ”€> Log metrics:
        â”‚   â”‚   mlflow.log_metric("cv_mean", cv_mean)
        â”‚   â”‚   mlflow.log_metric("cv_std", cv_std)
        â”‚   â”‚   mlflow.log_metric("test_accuracy", test_accuracy)
        â”‚   â”‚   mlflow.log_metric("test_f1", test_f1)
        â”‚   â”‚   mlflow.log_metric("test_precision", test_precision)
        â”‚   â”‚   mlflow.log_metric("test_recall", test_recall)
        â”‚   â”‚   mlflow.log_metric("training_time", training_time)
        â”‚   â”‚
        â”‚   â”œâ”€> Log model:
        â”‚   â”‚   signature = infer_signature(X_train, best_model.predict(X_train))
        â”‚   â”‚   mlflow.sklearn.log_model(
        â”‚   â”‚       sk_model=best_model,
        â”‚   â”‚       artifact_path="model",
        â”‚   â”‚       signature=signature,
        â”‚   â”‚       registered_model_name=None  # Register later after selection
        â”‚   â”‚   )
        â”‚   â”‚
        â”‚   â””â”€> Log artifacts:
        â”‚       â”œâ”€> mlflow.log_dict(cv_results_df.to_dict(), "cv_results.json")
        â”‚       â”œâ”€> mlflow.log_figure(confusion_matrix_plot, "confusion_matrix.png")
        â”‚       â””â”€> mlflow.log_figure(feature_importance_plot, "feature_importance.png")
        â”‚
        â”œâ”€> 8. END CHILD RUN
        â”‚   â””â”€> mlflow.end_run()
        â”‚
        â””â”€> 9. RETURN RESULT
            â””â”€> return AlgorithmResult(
                    model=best_model,
                    best_params=grid_search.best_params_,
                    cv_scores=cv_scores,
                    cv_mean=cv_mean,
                    cv_std=cv_std,
                    test_accuracy=test_accuracy,
                    test_f1=test_f1,
                    test_precision=test_precision,
                    test_recall=test_recall,
                    training_time=training_time,
                    mlflow_run_id=child_run_id,
                    predictions=y_pred
                )
```

### 3.3 Agent Decision Implementation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LEVEL 3: AI AGENT DECISION DETAILS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

class AlgorithmSelectionAgent(BaseBedrockAgent):
    def decide(state: PipelineState) -> AgentDecision:
        â”‚
        â”œâ”€> 1. CONTEXT PREPARATION
        â”‚   â”œâ”€> Extract state information:
        â”‚   â”‚   â”œâ”€> n_samples = len(state["X_train"])
        â”‚   â”‚   â”œâ”€> n_features = len(state["selected_features"])
        â”‚   â”‚   â”œâ”€> target_type = detect_target_type(state["y_train"])
        â”‚   â”‚   â”œâ”€> class_distribution = Counter(state["y_train"])
        â”‚   â”‚   â””â”€> feature_stats = state["feature_statistics"]
        â”‚   â”‚
        â”‚   â”œâ”€> Build context dictionary:
        â”‚   â”‚   context = {
        â”‚   â”‚       "n_samples": n_samples,
        â”‚   â”‚       "n_features": n_features,
        â”‚   â”‚       "target_type": target_type,
        â”‚   â”‚       "class_distribution": dict(class_distribution),
        â”‚   â”‚       "class_imbalance_ratio": max(class_distribution.values()) / min(class_distribution.values()),
        â”‚   â”‚       "feature_correlations": {
        â”‚   â”‚           "max_correlation": max(feature_stats["correlations"].values()),
        â”‚   â”‚           "multicollinear_pairs": [(f1, f2) for (f1, f2), corr in feature_stats["correlations"].items() if corr > 0.85]
        â”‚   â”‚       },
        â”‚   â”‚       "computational_budget": {
        â”‚   â”‚           "max_time_minutes": 60,
        â”‚   â”‚           "max_parallel_algorithms": 3,
        â”‚   â”‚           "available_memory_gb": 8
        â”‚   â”‚       }
        â”‚   â”‚   }
        â”‚   â”‚
        â”‚   â””â”€> Serialize to JSON:
        â”‚       context_json = json.dumps(context, indent=2)
        â”‚
        â”œâ”€> 2. PROMPT CONSTRUCTION
        â”‚   â”œâ”€> Load prompt template:
        â”‚   â”‚   template = load_template("algorithm_selection_prompt.txt")
        â”‚   â”‚
        â”‚   â”œâ”€> Template structure:
        â”‚   â”‚   """
        â”‚   â”‚   You are an expert ML engineer tasked with selecting algorithms.
        â”‚   â”‚
        â”‚   â”‚   Dataset characteristics:
        â”‚   â”‚   {context_json}
        â”‚   â”‚
        â”‚   â”‚   Available algorithms:
        â”‚   â”‚   - Random Forest
        â”‚   â”‚   - Gradient Boosting
        â”‚   â”‚   - Logistic Regression
        â”‚   â”‚   - SVM
        â”‚   â”‚   - KNN
        â”‚   â”‚
        â”‚   â”‚   Task: Analyze the dataset and select 2-4 algorithms that are
        â”‚   â”‚   most likely to perform well. Consider:
        â”‚   â”‚   - Dataset size
        â”‚   â”‚   - Feature types
        â”‚   â”‚   - Computational constraints
        â”‚   â”‚   - Training time
        â”‚   â”‚
        â”‚   â”‚   Output format (JSON):
        â”‚   â”‚   {
        â”‚   â”‚     "selected_algorithms": [...],
        â”‚   â”‚     "reasoning": {...},
        â”‚   â”‚     "hyperparameter_suggestions": {...},
        â”‚   â”‚     "skip_algorithms": [...],
        â”‚   â”‚     "skip_reasons": {...}
        â”‚   â”‚   }
        â”‚   â”‚   """
        â”‚   â”‚
        â”‚   â””â”€> Inject context:
        â”‚       prompt = template.format(context_json=context_json)
        â”‚
        â”œâ”€> 3. BEDROCK INVOCATION
        â”‚   â”œâ”€> Initialize retry handler:
        â”‚   â”‚   max_retries = 3
        â”‚   â”‚   attempt = 0
        â”‚   â”‚
        â”‚   â”œâ”€> Retry loop:
        â”‚   â”‚   while attempt < max_retries:
        â”‚   â”‚       try:
        â”‚   â”‚           â”œâ”€> Prepare request:
        â”‚   â”‚           â”‚   request_body = {
        â”‚   â”‚           â”‚       "anthropic_version": "bedrock-2023-05-31",
        â”‚   â”‚           â”‚       "max_tokens": 4096,
        â”‚   â”‚           â”‚       "temperature": self.temperature,
        â”‚   â”‚           â”‚       "messages": [
        â”‚   â”‚           â”‚           {
        â”‚   â”‚           â”‚               "role": "user",
        â”‚   â”‚           â”‚               "content": prompt
        â”‚   â”‚           â”‚           }
        â”‚   â”‚           â”‚       ]
        â”‚   â”‚           â”‚   }
        â”‚   â”‚           â”‚
        â”‚   â”‚           â”œâ”€> Invoke Bedrock:
        â”‚   â”‚           â”‚   response = self.bedrock_client.invoke_model(
        â”‚   â”‚           â”‚       modelId=self.model_id,
        â”‚   â”‚           â”‚       body=json.dumps(request_body)
        â”‚   â”‚           â”‚   )
        â”‚   â”‚           â”‚
        â”‚   â”‚           â”œâ”€> Parse response:
        â”‚   â”‚           â”‚   response_body = json.loads(response['body'].read())
        â”‚   â”‚           â”‚   raw_response = response_body['content'][0]['text']
        â”‚   â”‚           â”‚
        â”‚   â”‚           â””â”€> Break (success)
        â”‚   â”‚
        â”‚   â”‚       except RateLimitError as e:
        â”‚   â”‚           â”œâ”€> attempt += 1
        â”‚   â”‚           â”œâ”€> if attempt >= max_retries:
        â”‚   â”‚           â”‚   â””â”€> raise
        â”‚   â”‚           â”œâ”€> backoff_time = 2 ** attempt
        â”‚   â”‚           â”œâ”€> log.warning(f"Rate limited. Retrying in {backoff_time}s")
        â”‚   â”‚           â””â”€> time.sleep(backoff_time)
        â”‚   â”‚
        â”‚   â””â”€> Store: raw_response
        â”‚
        â”œâ”€> 4. RESPONSE PARSING
        â”‚   â”œâ”€> Extract JSON from response:
        â”‚   â”‚   â”œâ”€> If response contains ```json code blocks:
        â”‚   â”‚   â”‚   â”œâ”€> pattern = r'```json\n(.*?)\n```'
        â”‚   â”‚   â”‚   â””â”€> json_str = re.search(pattern, raw_response, re.DOTALL).group(1)
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€> else:
        â”‚   â”‚       â””â”€> json_str = raw_response
        â”‚   â”‚
        â”‚   â”œâ”€> Parse JSON:
        â”‚   â”‚   try:
        â”‚   â”‚       decision_dict = json.loads(json_str)
        â”‚   â”‚   except json.JSONDecodeError as e:
        â”‚   â”‚       â”œâ”€> log.error("Failed to parse agent response")
        â”‚   â”‚       â””â”€> raise
        â”‚   â”‚
        â”‚   â””â”€> Store: decision_dict
        â”‚
        â”œâ”€> 5. DECISION VALIDATION
        â”‚   â”œâ”€> Validate schema:
        â”‚   â”‚   required_keys = ["selected_algorithms", "reasoning"]
        â”‚   â”‚   for key in required_keys:
        â”‚   â”‚       assert key in decision_dict, f"Missing key: {key}"
        â”‚   â”‚
        â”‚   â”œâ”€> Validate selected_algorithms:
        â”‚   â”‚   â”œâ”€> assert isinstance(decision_dict["selected_algorithms"], list)
        â”‚   â”‚   â”œâ”€> assert 2 <= len(decision_dict["selected_algorithms"]) <= 4
        â”‚   â”‚   â””â”€> for algo in decision_dict["selected_algorithms"]:
        â”‚   â”‚       assert algo in SUPPORTED_ALGORITHMS
        â”‚   â”‚
        â”‚   â””â”€> If validation fails:
        â”‚       â””â”€> log.error() and raise ValidationError
        â”‚
        â”œâ”€> 6. MLFLOW LOGGING
        â”‚   â”œâ”€> mlflow.log_param("agent1_model", self.model_id)
        â”‚   â”œâ”€> mlflow.log_param("agent1_temperature", self.temperature)
        â”‚   â”‚
        â”‚   â””â”€> mlflow.log_dict({
        â”‚           "context": context,
        â”‚           "prompt": prompt,
        â”‚           "raw_response": raw_response,
        â”‚           "decision": decision_dict
        â”‚       }, "agent_decisions/algorithm_selection.json")
        â”‚
        â”œâ”€> 7. CREATE AGENT DECISION OBJECT
        â”‚   â””â”€> agent_decision = AgentDecision(
        â”‚           decision=decision_dict,
        â”‚           reasoning=decision_dict["reasoning"],
        â”‚           confidence=decision_dict.get("confidence", 0.9),
        â”‚           prompt=prompt,
        â”‚           raw_response=raw_response,
        â”‚           timestamp=datetime.now()
        â”‚       )
        â”‚
        â””â”€> 8. RETURN DECISION
            â””â”€> return agent_decision
```

### 3.4 Drift Detection Implementation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LEVEL 3: DRIFT DETECTION DETAILS                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

class DriftDetector:
    def detect_drift(reference_data, current_data) -> DriftReport:
        â”‚
        â”œâ”€> 1. DATA PREPARATION
        â”‚   â”œâ”€> Validate inputs:
        â”‚   â”‚   â”œâ”€> assert reference_data.shape[1] == current_data.shape[1]
        â”‚   â”‚   â””â”€> assert list(reference_data.columns) == list(current_data.columns)
        â”‚   â”‚
        â”‚   â””â”€> Initialize results storage:
        â”‚       drift_results = {}
        â”‚
        â”œâ”€> 2. FEATURE-WISE DRIFT DETECTION
        â”‚   â”œâ”€> For each column in reference_data.columns:
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> Determine feature type:
        â”‚   â”‚   â”‚   if pd.api.types.is_numeric_dtype(reference_data[col]):
        â”‚   â”‚   â”‚       feature_type = "numerical"
        â”‚   â”‚   â”‚   else:
        â”‚   â”‚   â”‚       feature_type = "categorical"
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> NUMERICAL DRIFT (Kolmogorov-Smirnov Test):
        â”‚   â”‚   â”‚   if feature_type == "numerical":
        â”‚   â”‚   â”‚       â”œâ”€> from scipy.stats import ks_2samp
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â”œâ”€> Perform KS test:
        â”‚   â”‚   â”‚       â”‚   statistic, p_value = ks_2samp(
        â”‚   â”‚   â”‚       â”‚       reference_data[col].values,
        â”‚   â”‚   â”‚       â”‚       current_data[col].values
        â”‚   â”‚   â”‚       â”‚   )
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â”œâ”€> Interpret result:
        â”‚   â”‚   â”‚       â”‚   if p_value < 0.05:
        â”‚   â”‚   â”‚       â”‚       drift_detected = True
        â”‚   â”‚   â”‚       â”‚       # Null hypothesis rejected: distributions differ
        â”‚   â”‚   â”‚       â”‚   else:
        â”‚   â”‚   â”‚       â”‚       drift_detected = False
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â””â”€> Store result:
        â”‚   â”‚   â”‚           drift_results[col] = {
        â”‚   â”‚   â”‚               "test": "ks_2samp",
        â”‚   â”‚   â”‚               "statistic": statistic,
        â”‚   â”‚   â”‚               "p_value": p_value,
        â”‚   â”‚   â”‚               "drift_detected": drift_detected
        â”‚   â”‚   â”‚           }
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> CATEGORICAL DRIFT (Chi-Squared Test):
        â”‚   â”‚   â”‚   elif feature_type == "categorical":
        â”‚   â”‚   â”‚       â”œâ”€> from scipy.stats import chi2_contingency
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â”œâ”€> Create contingency table:
        â”‚   â”‚   â”‚       â”‚   ref_counts = reference_data[col].value_counts()
        â”‚   â”‚   â”‚       â”‚   cur_counts = current_data[col].value_counts()
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â”‚   # Align categories
        â”‚   â”‚   â”‚       â”‚   all_categories = set(ref_counts.index) | set(cur_counts.index)
        â”‚   â”‚   â”‚       â”‚   ref_aligned = [ref_counts.get(cat, 0) for cat in all_categories]
        â”‚   â”‚   â”‚       â”‚   cur_aligned = [cur_counts.get(cat, 0) for cat in all_categories]
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â”‚   contingency_table = np.array([ref_aligned, cur_aligned])
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â”œâ”€> Perform ChiÂ² test:
        â”‚   â”‚   â”‚       â”‚   chi2, p_value, dof, expected = chi2_contingency(contingency_table)
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â”œâ”€> Interpret result:
        â”‚   â”‚   â”‚       â”‚   if p_value < 0.05:
        â”‚   â”‚   â”‚       â”‚       drift_detected = True
        â”‚   â”‚   â”‚       â”‚   else:
        â”‚   â”‚   â”‚       â”‚       drift_detected = False
        â”‚   â”‚   â”‚       â”‚
        â”‚   â”‚   â”‚       â””â”€> Store result:
        â”‚   â”‚   â”‚           drift_results[col] = {
        â”‚   â”‚   â”‚               "test": "chi2",
        â”‚   â”‚   â”‚               "statistic": chi2,
        â”‚   â”‚   â”‚               "p_value": p_value,
        â”‚   â”‚   â”‚               "drift_detected": drift_detected
        â”‚   â”‚   â”‚           }
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€> POPULATION STABILITY INDEX (PSI):
        â”‚   â”‚       â”œâ”€> Bin data:
        â”‚   â”‚       â”‚   if feature_type == "numerical":
        â”‚   â”‚       â”‚       â”œâ”€> bins = np.linspace(
        â”‚   â”‚       â”‚       â”‚       reference_data[col].min(),
        â”‚   â”‚       â”‚       â”‚       reference_data[col].max(),
        â”‚   â”‚       â”‚       â”‚       num=10
        â”‚   â”‚       â”‚       â”‚   )
        â”‚   â”‚       â”‚       â”œâ”€> ref_binned = pd.cut(reference_data[col], bins=bins)
        â”‚   â”‚       â”‚       â””â”€> cur_binned = pd.cut(current_data[col], bins=bins)
        â”‚   â”‚       â”‚   else:
        â”‚   â”‚       â”‚       â”œâ”€> ref_binned = reference_data[col]
        â”‚   â”‚       â”‚       â””â”€> cur_binned = current_data[col]
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€> Calculate distributions:
        â”‚   â”‚       â”‚   ref_dist = ref_binned.value_counts(normalize=True)
        â”‚   â”‚       â”‚   cur_dist = cur_binned.value_counts(normalize=True)
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€> Align distributions:
        â”‚   â”‚       â”‚   all_bins = set(ref_dist.index) | set(cur_dist.index)
        â”‚   â”‚       â”‚   ref_pct = [ref_dist.get(b, 0.0001) for b in all_bins]  # Small epsilon
        â”‚   â”‚       â”‚   cur_pct = [cur_dist.get(b, 0.0001) for b in all_bins]
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€> Calculate PSI:
        â”‚   â”‚       â”‚   psi = sum([
        â”‚   â”‚       â”‚       (cur - ref) * np.log(cur / ref)
        â”‚   â”‚       â”‚       for cur, ref in zip(cur_pct, ref_pct)
        â”‚   â”‚       â”‚   ])
        â”‚   â”‚       â”‚
        â”‚   â”‚       â”œâ”€> Interpret PSI:
        â”‚   â”‚       â”‚   if psi < 0.1:
        â”‚   â”‚       â”‚       psi_interpretation = "No significant shift"
        â”‚   â”‚       â”‚   elif psi < 0.25:
        â”‚   â”‚       â”‚       psi_interpretation = "Moderate shift"
        â”‚   â”‚       â”‚   else:
        â”‚   â”‚       â”‚       psi_interpretation = "Significant shift"
        â”‚   â”‚       â”‚
        â”‚   â”‚       â””â”€> Add to results:
        â”‚   â”‚           drift_results[col]["psi"] = psi
        â”‚   â”‚           drift_results[col]["psi_interpretation"] = psi_interpretation
        â”‚   â”‚
        â”‚   â””â”€> End for loop
        â”‚
        â”œâ”€> 3. AGGREGATE DRIFT ANALYSIS
        â”‚   â”œâ”€> Calculate overall drift score:
        â”‚   â”‚   â”œâ”€> Count drifted features:
        â”‚   â”‚   â”‚   drifted_features = [
        â”‚   â”‚   â”‚       col for col, result in drift_results.items()
        â”‚   â”‚   â”‚       if result["drift_detected"] or result.get("psi", 0) >= 0.25
        â”‚   â”‚   â”‚   ]
        â”‚   â”‚   â”‚
        â”‚   â”‚   â”œâ”€> Calculate aggregate score:
        â”‚   â”‚   â”‚   # Weighted average of PSI values
        â”‚   â”‚   â”‚   psi_values = [result.get("psi", 0) for result in drift_results.values()]
        â”‚   â”‚   â”‚   overall_drift_score = np.mean(psi_values)
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€> Determine if drift detected:
        â”‚   â”‚       drift_detected = (
        â”‚   â”‚           len(drifted_features) > 0 or
        â”‚   â”‚           overall_drift_score >= self.drift_threshold
        â”‚   â”‚       )
        â”‚   â”‚
        â”‚   â””â”€> Create drift report:
        â”‚       drift_report = DriftReport(
        â”‚           drift_detected=drift_detected,
        â”‚           overall_drift_score=overall_drift_score,
        â”‚           drifted_features=drifted_features,
        â”‚           feature_drift_scores={
        â”‚               col: result.get("psi", result.get("statistic", 0))
        â”‚               for col, result in drift_results.items()
        â”‚           },
        â”‚           psi_values={
        â”‚               col: result.get("psi", 0)
        â”‚               for col, result in drift_results.items()
        â”‚           },
        â”‚           drift_details=drift_results,
        â”‚           timestamp=datetime.now()
        â”‚       )
        â”‚
        â”œâ”€> 4. VISUALIZATION
        â”‚   â”œâ”€> Create drift visualization:
        â”‚   â”‚   â”œâ”€> Plot 1: PSI bar chart
        â”‚   â”‚   â”‚   fig, ax = plt.subplots()
        â”‚   â”‚   â”‚   ax.bar(psi_values.keys(), psi_values.values())
        â”‚   â”‚   â”‚   ax.axhline(y=0.25, color='r', linestyle='--', label='Threshold')
        â”‚   â”‚   â”‚   plt.savefig("drift_psi.png")
        â”‚   â”‚   â”‚
        â”‚   â”‚   â””â”€> Plot 2: Distribution overlays for drifted features
        â”‚   â”‚       for col in drifted_features[:5]:  # Top 5
        â”‚   â”‚           fig, ax = plt.subplots()
        â”‚   â”‚           ax.hist(reference_data[col], alpha=0.5, label='Reference')
        â”‚   â”‚           ax.hist(current_data[col], alpha=0.5, label='Current')
        â”‚   â”‚           ax.legend()
        â”‚   â”‚           plt.savefig(f"drift_{col}.png")
        â”‚   â”‚
        â”‚   â””â”€> Store: visualization_paths
        â”‚
        â”œâ”€> 5. MLFLOW LOGGING
        â”‚   â”œâ”€> mlflow.log_metrics({
        â”‚   â”‚       "drift_score": overall_drift_score,
        â”‚   â”‚       "psi_max": max(psi_values.values()),
        â”‚   â”‚       "drift_features_count": len(drifted_features)
        â”‚   â”‚   })
        â”‚   â”‚
        â”‚   â””â”€> mlflow.log_artifacts:
        â”‚       â”œâ”€> mlflow.log_dict(drift_report.to_dict(), "drift_report.json")
        â”‚       â”œâ”€> mlflow.log_figure(drift_psi_plot, "drift_psi.png")
        â”‚       â””â”€> for viz_path in visualization_paths:
        â”‚           mlflow.log_artifact(viz_path)
        â”‚
        â””â”€> 6. RETURN REPORT
            â””â”€> return drift_report
```

### 3.5 Algorithm-Aware HITL Implementation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LEVEL 3: ALGORITHM-AWARE HITL SYSTEM IMPLEMENTATION DETAILS      â”‚
â”‚    (Two-Agent Architecture with Dynamic Question Generation)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: ALGORITHM CATEGORY PREDICTION (Agent 1A)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AlgorithmCategoryPredictor:
    def predict_category(state: PipelineState) -> AlgorithmPrediction:
        â”‚
        â”œâ”€> 1. EXTRACT DATA PROFILE
        â”‚   â”œâ”€> From state["raw_data"]:
        â”‚   â”‚   n_samples = len(raw_data)
        â”‚   â”‚   n_features = len(raw_data.columns) - 1  # excluding target
        â”‚   â”‚   target_column = state["pipeline_config"]["target_column"]
        â”‚   â”‚
        â”‚   â”œâ”€> Determine target_type:
        â”‚   â”‚   if raw_data[target_column].dtype in ['object', 'category']:
        â”‚   â”‚       unique_count = raw_data[target_column].nunique()
        â”‚   â”‚       if unique_count == 2:
        â”‚   â”‚           target_type = "binary_classification"
        â”‚   â”‚       else:
        â”‚   â”‚           target_type = "multiclass_classification"
        â”‚   â”‚   elif raw_data[target_column].dtype in ['int64', 'float64']:
        â”‚   â”‚       target_type = "regression"
        â”‚   â”‚
        â”‚   â”œâ”€> Analyze feature types:
        â”‚   â”‚   numeric_count = len(raw_data.select_dtypes(include=[np.number]).columns) - 1
        â”‚   â”‚   categorical_count = len(raw_data.select_dtypes(include=['object','category']).columns)
        â”‚   â”‚   high_cardinality_count = sum(1 for col in categorical_cols
        â”‚   â”‚                                  if raw_data[col].nunique() > 10)
        â”‚   â”‚
        â”‚   â”œâ”€> Calculate data characteristics:
        â”‚   â”‚   missing_percentage = (raw_data.isnull().sum().sum() /
        â”‚   â”‚                         (n_samples * n_features)) * 100
        â”‚   â”‚   duplicate_percentage = (raw_data.duplicated().sum() / n_samples) * 100
        â”‚   â”‚
        â”‚   â”‚   # Outlier detection (quick IQR check)
        â”‚   â”‚   numeric_data = raw_data.select_dtypes(include=[np.number])
        â”‚   â”‚   outlier_counts = 0
        â”‚   â”‚   for col in numeric_data.columns:
        â”‚   â”‚       Q1, Q3 = numeric_data[col].quantile([0.25, 0.75])
        â”‚   â”‚       IQR = Q3 - Q1
        â”‚   â”‚       outlier_counts += ((numeric_data[col] < (Q1 - 1.5*IQR)) |
        â”‚   â”‚                          (numeric_data[col] > (Q3 + 1.5*IQR))).sum()
        â”‚   â”‚   outlier_percentage = (outlier_counts / (n_samples * numeric_count)) * 100
        â”‚   â”‚
        â”‚   â”‚   # Correlation analysis
        â”‚   â”‚   corr_matrix = numeric_data.corr().abs()
        â”‚   â”‚   feature_correlation_max = corr_matrix[corr_matrix < 1.0].max().max()
        â”‚   â”‚
        â”‚   â””â”€> Build data_profile dict:
        â”‚       data_profile = {
        â”‚           "n_samples": n_samples,
        â”‚           "n_features": n_features,
        â”‚           "target_type": target_type,
        â”‚           "feature_types": {
        â”‚               "numeric_count": numeric_count,
        â”‚               "categorical_count": categorical_count,
        â”‚               "high_cardinality_count": high_cardinality_count
        â”‚           },
        â”‚           "class_distribution": raw_data[target_column].value_counts().to_dict(),
        â”‚           "dataset_size_mb": raw_data.memory_usage(deep=True).sum() / 1024**2,
        â”‚           "data_characteristics": {
        â”‚               "missing_percentage": missing_percentage,
        â”‚               "duplicate_percentage": duplicate_percentage,
        â”‚               "outlier_percentage": outlier_percentage,
        â”‚               "feature_correlation_max": feature_correlation_max
        â”‚           }
        â”‚       }
        â”‚
        â”œâ”€> 2. BUILD BEDROCK PROMPT
        â”‚   â”œâ”€> Load prompt template from PROMPT.md section
        â”‚   â”‚   (Agent 1A: Algorithm Category Predictor)
        â”‚   â”‚
        â”‚   â”œâ”€> Format prompt with data_profile:
        â”‚   â”‚   prompt = f"""
        â”‚   â”‚   You are an ML algorithm expert. Analyze this dataset profile and predict
        â”‚   â”‚   the optimal algorithm category.
        â”‚   â”‚
        â”‚   â”‚   Dataset Profile:
        â”‚   â”‚   {json.dumps(data_profile, indent=2)}
        â”‚   â”‚
        â”‚   â”‚   Predict the algorithm category from:
        â”‚   â”‚   - linear_models (LogisticRegression, LinearRegression, Ridge)
        â”‚   â”‚   - tree_models (RandomForest, XGBoost, GradientBoosting)
        â”‚   â”‚   - neural_networks (MLP, Deep Learning models)
        â”‚   â”‚   - ensemble (Stacking, Voting, Blending)
        â”‚   â”‚   - time_series (ARIMA, LSTM, Prophet)
        â”‚   â”‚
        â”‚   â”‚   Return JSON:
        â”‚   â”‚   {{
        â”‚   â”‚     "algorithm_category": "tree_models",
        â”‚   â”‚     "confidence": 0.87,
        â”‚   â”‚     "reasoning": "...",
        â”‚   â”‚     "recommended_algorithms": ["RandomForest", "XGBoost"],
        â”‚   â”‚     "preprocessing_priorities": {{
        â”‚   â”‚       "clean_data": "optional",
        â”‚   â”‚       "handle_missing": "required",
        â”‚   â”‚       "encode_features": "required",
        â”‚   â”‚       "scale_features": "optional"
        â”‚   â”‚     }},
        â”‚   â”‚     "algorithm_requirements": {{
        â”‚   â”‚       "scaling_required": false,
        â”‚   â”‚       "outlier_sensitive": false,
        â”‚   â”‚       "handles_missing": true,
        â”‚   â”‚       "categorical_encoding_preference": "target"
        â”‚   â”‚     }}
        â”‚   â”‚   }}
        â”‚   â”‚   """
        â”‚   â”‚
        â”‚   â””â”€> Store prompt_text
        â”‚
        â”œâ”€> 3. INVOKE BEDROCK CLAUDE
        â”‚   â”œâ”€> Configure Bedrock client:
        â”‚   â”‚   import boto3
        â”‚   â”‚   bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')
        â”‚   â”‚
        â”‚   â”œâ”€> Prepare request:
        â”‚   â”‚   body = json.dumps({
        â”‚   â”‚       "anthropic_version": "bedrock-2023-05-31",
        â”‚   â”‚       "max_tokens": 2000,
        â”‚   â”‚       "temperature": 0.2,  # Low for consistent predictions
        â”‚   â”‚       "messages": [{
        â”‚   â”‚           "role": "user",
        â”‚   â”‚           "content": prompt_text
        â”‚   â”‚       }]
        â”‚   â”‚   })
        â”‚   â”‚
        â”‚   â”œâ”€> Invoke with retry logic:
        â”‚   â”‚   for attempt in range(3):
        â”‚   â”‚       try:
        â”‚   â”‚           response = bedrock_runtime.invoke_model(
        â”‚   â”‚               modelId="us.anthropic.claude-sonnet-4-5-20250929-v1:0",
        â”‚   â”‚               body=body
        â”‚   â”‚           )
        â”‚   â”‚           break
        â”‚   â”‚       except botocore.exceptions.ClientError as e:
        â”‚   â”‚           if e.response['Error']['Code'] == 'ThrottlingException':
        â”‚   â”‚               time.sleep(2 ** attempt)  # Exponential backoff
        â”‚   â”‚           else:
        â”‚   â”‚               raise
        â”‚   â”‚
        â”‚   â””â”€> Extract response:
        â”‚       response_body = json.loads(response['body'].read())
        â”‚       response_text = response_body['content'][0]['text']
        â”‚
        â”œâ”€> 4. PARSE RESPONSE
        â”‚   â”œâ”€> Extract JSON from response_text:
        â”‚   â”‚   import re
        â”‚   â”‚   json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        â”‚   â”‚   decision_json = json.loads(json_match.group())
        â”‚   â”‚
        â”‚   â”œâ”€> Validate schema:
        â”‚   â”‚   required_keys = ["algorithm_category", "confidence", "reasoning",
        â”‚   â”‚                    "recommended_algorithms", "preprocessing_priorities",
        â”‚   â”‚                    "algorithm_requirements"]
        â”‚   â”‚   assert all(key in decision_json for key in required_keys)
        â”‚   â”‚   assert decision_json["confidence"] >= 0.70  # Minimum threshold
        â”‚   â”‚
        â”‚   â””â”€> Create AlgorithmPrediction object:
        â”‚       prediction = AlgorithmPrediction(
        â”‚           algorithm_category=decision_json["algorithm_category"],
        â”‚           confidence=decision_json["confidence"],
        â”‚           reasoning=decision_json["reasoning"],
        â”‚           recommended_algorithms=decision_json["recommended_algorithms"],
        â”‚           preprocessing_priorities=decision_json["preprocessing_priorities"],
        â”‚           algorithm_requirements=decision_json["algorithm_requirements"]
        â”‚       )
        â”‚
        â”œâ”€> 5. STORE IN POSTGRESQL
        â”‚   â”œâ”€> INSERT INTO review_questions:
        â”‚   â”‚   sql = """
        â”‚   â”‚   INSERT INTO review_questions
        â”‚   â”‚   (pipeline_id, algorithm_category, algorithm_confidence,
        â”‚   â”‚    algorithm_requirements, agent_1a_prompt, agent_1a_response,
        â”‚   â”‚    created_at)
        â”‚   â”‚   VALUES (%s, %s, %s, %s, %s, %s, NOW())
        â”‚   â”‚   RETURNING id
        â”‚   â”‚   """
        â”‚   â”‚   cursor.execute(sql, (pipeline_id, prediction.algorithm_category,
        â”‚   â”‚                        prediction.confidence,
        â”‚   â”‚                        json.dumps(prediction.algorithm_requirements),
        â”‚   â”‚                        prompt_text, response_text))
        â”‚   â”‚   review_session_id = cursor.fetchone()[0]
        â”‚   â”‚
        â”‚   â””â”€> commit()
        â”‚
        â”œâ”€> 6. LOG TO MLFLOW
        â”‚   â”œâ”€> mlflow.log_params({
        â”‚   â”‚       "algorithm_category": prediction.algorithm_category,
        â”‚   â”‚       "algorithm_confidence": prediction.confidence,
        â”‚   â”‚       "agent_1a_model": "claude-sonnet-4-5",
        â”‚   â”‚       "agent_1a_temperature": 0.2
        â”‚   â”‚   })
        â”‚   â”‚
        â”‚   â””â”€> mlflow.log_artifacts:
        â”‚       â”œâ”€> mlflow.log_text(prompt_text, "agents/agent_1a_prompt.txt")
        â”‚       â”œâ”€> mlflow.log_text(response_text, "agents/agent_1a_response.txt")
        â”‚       â””â”€> mlflow.log_dict(decision_json, "agents/agent_1a_prediction.json")
        â”‚
        â””â”€> 7. UPDATE STATE
            â””â”€> state.update({
                    "algorithm_category": prediction.algorithm_category,
                    "algorithm_confidence": prediction.confidence,
                    "algorithm_requirements": prediction.algorithm_requirements,
                    "preprocessing_priorities": prediction.preprocessing_priorities,
                    "recommended_algorithms": prediction.recommended_algorithms,
                    "review_session_id": review_session_id
                })


STEP 2: PREPROCESSING QUESTION GENERATION (Agent 1B)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PreprocessingQuestionGenerator:
    def generate_questions(state: PipelineState) -> List[Question]:
        â”‚
        â”œâ”€> 1. BUILD ALGORITHM-AWARE CONTEXT
        â”‚   â”œâ”€> Extract from state:
        â”‚   â”‚   algorithm_category = state["algorithm_category"]
        â”‚   â”‚   algorithm_confidence = state["algorithm_confidence"]
        â”‚   â”‚   algorithm_requirements = state["algorithm_requirements"]
        â”‚   â”‚   data_profile = state["data_profile"]  # From Agent 1A
        â”‚   â”‚   preprocessing_steps = ["clean_data", "handle_missing",
        â”‚   â”‚                           "encode_features", "scale_features"]
        â”‚   â”‚
        â”‚   â”œâ”€> Enhance data_profile with detailed statistics:
        â”‚   â”‚   raw_data = state["raw_data"]
        â”‚   â”‚
        â”‚   â”‚   # Missing values per column
        â”‚   â”‚   missing_values = {col: int(raw_data[col].isnull().sum())
        â”‚   â”‚                     for col in raw_data.columns
        â”‚   â”‚                     if raw_data[col].isnull().sum() > 0}
        â”‚   â”‚
        â”‚   â”‚   # Categorical columns
        â”‚   â”‚   categorical_columns = raw_data.select_dtypes(
        â”‚   â”‚       include=['object','category']).columns.tolist()
        â”‚   â”‚
        â”‚   â”‚   # High cardinality columns
        â”‚   â”‚   high_cardinality_columns = [col for col in categorical_columns
        â”‚   â”‚                                if raw_data[col].nunique() > 10]
        â”‚   â”‚
        â”‚   â”‚   # Outlier summary per numeric column
        â”‚   â”‚   outlier_summary = {}
        â”‚   â”‚   for col in raw_data.select_dtypes(include=[np.number]).columns:
        â”‚   â”‚       Q1 = raw_data[col].quantile(0.25)
        â”‚   â”‚       Q3 = raw_data[col].quantile(0.75)
        â”‚   â”‚       IQR = Q3 - Q1
        â”‚   â”‚       outlier_count = ((raw_data[col] < (Q1 - 1.5*IQR)) |
        â”‚   â”‚                        (raw_data[col] > (Q3 + 1.5*IQR))).sum()
        â”‚   â”‚       if outlier_count > 0:
        â”‚   â”‚           outlier_summary[col] = {
        â”‚   â”‚               "count": int(outlier_count),
        â”‚   â”‚               "Q1": float(Q1),
        â”‚   â”‚               "Q3": float(Q3),
        â”‚   â”‚               "IQR": float(IQR)
        â”‚   â”‚           }
        â”‚   â”‚
        â”‚   â”‚   data_profile["missing_values"] = missing_values
        â”‚   â”‚   data_profile["duplicate_rows"] = int(raw_data.duplicated().sum())
        â”‚   â”‚   data_profile["categorical_columns"] = categorical_columns
        â”‚   â”‚   data_profile["high_cardinality_columns"] = high_cardinality_columns
        â”‚   â”‚   data_profile["outlier_summary"] = outlier_summary
        â”‚   â”‚
        â”‚   â””â”€> Build context dict:
        â”‚       context = {
        â”‚           "algorithm_category": algorithm_category,
        â”‚           "algorithm_confidence": algorithm_confidence,
        â”‚           "algorithm_requirements": algorithm_requirements,
        â”‚           "data_profile": data_profile,
        â”‚           "preprocessing_steps": preprocessing_steps
        â”‚       }
        â”‚
        â”œâ”€> 2. BUILD BEDROCK PROMPT
        â”‚   â”œâ”€> Load prompt template from PROMPT.md section
        â”‚   â”‚   (Agent 1B: Preprocessing Question Generator)
        â”‚   â”‚
        â”‚   â”œâ”€> Format prompt with context:
        â”‚   â”‚   prompt = f"""
        â”‚   â”‚   You are a preprocessing expert. Generate algorithm-aware preprocessing
        â”‚   â”‚   questions based on the predicted algorithm category and dataset profile.
        â”‚   â”‚
        â”‚   â”‚   Algorithm Context:
        â”‚   â”‚   {json.dumps(context, indent=2)}
        â”‚   â”‚
        â”‚   â”‚   Generate 1-5 questions per preprocessing step (4-20 total).
        â”‚   â”‚   Question count should be based on:
        â”‚   â”‚   - Algorithm requirements (from algorithm_requirements)
        â”‚   â”‚   - Data characteristics (missing%, outliers, cardinality)
        â”‚   â”‚   - Preprocessing priorities (from Agent 1A)
        â”‚   â”‚
        â”‚   â”‚   For each question, provide:
        â”‚   â”‚   - question_id, preprocessing_step, question_text, question_type, priority
        â”‚   â”‚   - options: [{{value, label, recommended, reasoning, algorithm_suitability}}]
        â”‚   â”‚   - parameters: [{{param_name, param_type, default, range, description}}]
        â”‚   â”‚
        â”‚   â”‚   Rank technique options by algorithm suitability:
        â”‚   â”‚   - "excellent": Optimal for {algorithm_category}
        â”‚   â”‚   - "good": Works well with {algorithm_category}
        â”‚   â”‚   - "acceptable": Can work but not ideal
        â”‚   â”‚   - "poor": Not recommended for {algorithm_category}
        â”‚   â”‚
        â”‚   â”‚   Return JSON with "questions", "preprocessing_recommendations",
        â”‚   â”‚   "algorithm_context", "question_count_by_step".
        â”‚   â”‚   """
        â”‚   â”‚
        â”‚   â””â”€> Store prompt_text
        â”‚
        â”œâ”€> 3. INVOKE BEDROCK CLAUDE
        â”‚   â”œâ”€> Configure with temperature=0.3 (slightly higher for natural phrasing)
        â”‚   â”‚
        â”‚   â””â”€> Same invocation logic as Agent 1A (with retry)
        â”‚
        â”œâ”€> 4. PARSE RESPONSE
        â”‚   â”œâ”€> Extract JSON and validate:
        â”‚   â”‚   questions_json = json.loads(json_match.group())
        â”‚   â”‚   assert "questions" in questions_json
        â”‚   â”‚   assert 4 <= len(questions_json["questions"]) <= 20
        â”‚   â”‚
        â”‚   â”‚   # Validate each question structure
        â”‚   â”‚   for question in questions_json["questions"]:
        â”‚   â”‚       assert all(key in question for key in
        â”‚   â”‚                  ["question_id", "preprocessing_step", "question_text",
        â”‚   â”‚                   "question_type", "priority", "options"])
        â”‚   â”‚       assert question["preprocessing_step"] in preprocessing_steps
        â”‚   â”‚       assert question["priority"] in ["low", "medium", "high"]
        â”‚   â”‚
        â”‚   â”‚       # Validate options
        â”‚   â”‚       for option in question["options"]:
        â”‚   â”‚           assert all(key in option for key in
        â”‚   â”‚                      ["value", "label", "recommended"])
        â”‚   â”‚           if "algorithm_suitability" in option:
        â”‚   â”‚               assert option["algorithm_suitability"] in
        â”‚   â”‚                      ["excellent", "good", "acceptable", "poor"]
        â”‚   â”‚
        â”‚   â””â”€> Create Question objects
        â”‚
        â”œâ”€> 5. STORE IN POSTGRESQL
        â”‚   â”œâ”€> UPDATE review_questions SET:
        â”‚   â”‚   agent_1b_prompt = prompt_text,
        â”‚   â”‚   agent_1b_response = response_text,
        â”‚   â”‚   questions = json.dumps(questions_json["questions"]),
        â”‚   â”‚   preprocessing_recommendations = json.dumps(
        â”‚   â”‚       questions_json["preprocessing_recommendations"]),
        â”‚   â”‚   question_count = len(questions_json["questions"]),
        â”‚   â”‚   question_count_by_step = json.dumps(
        â”‚   â”‚       questions_json["question_count_by_step"])
        â”‚   â”‚   WHERE id = state["review_session_id"]
        â”‚   â”‚
        â”‚   â””â”€> commit()
        â”‚
        â”œâ”€> 6. LOG TO MLFLOW
        â”‚   â”œâ”€> mlflow.log_params({
        â”‚   â”‚       "question_count": len(questions_json["questions"]),
        â”‚   â”‚       "question_count_by_step": json.dumps(
        â”‚   â”‚           questions_json["question_count_by_step"]),
        â”‚   â”‚       "agent_1b_model": "claude-sonnet-4-5",
        â”‚   â”‚       "agent_1b_temperature": 0.3
        â”‚   â”‚   })
        â”‚   â”‚
        â”‚   â””â”€> mlflow.log_artifacts:
        â”‚       â”œâ”€> mlflow.log_text(prompt_text, "agents/agent_1b_prompt.txt")
        â”‚       â”œâ”€> mlflow.log_text(response_text, "agents/agent_1b_response.txt")
        â”‚       â””â”€> mlflow.log_dict(questions_json, "agents/agent_1b_questions.json")
        â”‚
        â””â”€> 7. UPDATE STATE & TRIGGER INTERRUPTION
            â”œâ”€> state.update({
            â”‚       "review_questions": questions_json["questions"],
            â”‚       "question_count": len(questions_json["questions"]),
            â”‚       "question_count_by_step": questions_json["question_count_by_step"],
            â”‚       "preprocessing_recommendations":
            â”‚           questions_json["preprocessing_recommendations"],
            â”‚       "status": "awaiting_review"
            â”‚   })
            â”‚
            â””â”€> workflow.interrupt_after(["review_config"])
                # LangGraph pauses execution here


STEP 3: TECHNIQUE-BASED PREPROCESSING EXECUTION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TechniqueBasedPreprocessor:
    def execute_preprocessing(state: PipelineState) -> PipelineState:
        â”‚
        â”œâ”€> 1. EXTRACT USER SELECTIONS
        â”‚   â”œâ”€> From state["review_answers"]:
        â”‚   â”‚   clean_data_technique = review_answers.get("clean_data_technique")
        â”‚   â”‚   handle_missing_technique = review_answers.get("handle_missing_technique")
        â”‚   â”‚   encode_technique = review_answers.get("encode_technique")
        â”‚   â”‚   scale_technique = review_answers.get("scale_technique")
        â”‚   â”‚
        â”‚   â”‚   # Extract technique-specific parameters
        â”‚   â”‚   technique_parameters = {}
        â”‚   â”‚   for key, value in review_answers.items():
        â”‚   â”‚       if key.endswith("_technique"):
        â”‚   â”‚           continue
        â”‚   â”‚       technique_parameters[key] = value
        â”‚   â”‚
        â”‚   â””â”€> Example technique_parameters:
        â”‚       {
        â”‚           "imputation_strategy": "median",
        â”‚           "knn_neighbors": 5,
        â”‚           "target_smoothing": 1.0,
        â”‚           "iqr_multiplier": 1.5
        â”‚       }
        â”‚
        â”œâ”€> 2. EXECUTE CLEAN_DATA TECHNIQUE
        â”‚   â”œâ”€> Load technique registry:
        â”‚   â”‚   from techniques.clean_data import TECHNIQUES
        â”‚   â”‚   technique_func = TECHNIQUES[clean_data_technique]
        â”‚   â”‚
        â”‚   â”œâ”€> Execute selected technique:
        â”‚   â”‚   if clean_data_technique == "none":
        â”‚   â”‚       cleaned_data = raw_data.copy()
        â”‚   â”‚       outliers_removed = 0
        â”‚   â”‚   elif clean_data_technique == "iqr_method":
        â”‚   â”‚       multiplier = technique_parameters.get("iqr_multiplier", 1.5)
        â”‚   â”‚       cleaned_data, outliers_removed = technique_func(
        â”‚   â”‚           raw_data, multiplier=multiplier)
        â”‚   â”‚   elif clean_data_technique == "z_score":
        â”‚   â”‚       threshold = technique_parameters.get("z_score_threshold", 3.0)
        â”‚   â”‚       cleaned_data, outliers_removed = technique_func(
        â”‚   â”‚           raw_data, threshold=threshold)
        â”‚   â”‚   # ... other techniques (winsorization, isolation_forest, etc.)
        â”‚   â”‚
        â”‚   â””â”€> Log to MLflow:
        â”‚       mlflow.log_params({
        â”‚           "clean_data_technique": clean_data_technique,
        â”‚           **{k: v for k, v in technique_parameters.items()
        â”‚              if k.startswith("clean_") or k in ["iqr_multiplier", "z_score_threshold"]}
        â”‚       })
        â”‚       mlflow.log_metrics({"outliers_removed": outliers_removed})
        â”‚
        â”œâ”€> 3. EXECUTE HANDLE_MISSING TECHNIQUE
        â”‚   â”œâ”€> Execute based on selection:
        â”‚   â”‚   if handle_missing_technique == "simple_imputation":
        â”‚   â”‚       strategy = technique_parameters.get("imputation_strategy", "median")
        â”‚   â”‚       imputer = SimpleImputer(strategy=strategy)
        â”‚   â”‚       imputed_data = imputer.fit_transform(cleaned_data)
        â”‚   â”‚   elif handle_missing_technique == "knn_imputation":
        â”‚   â”‚       n_neighbors = technique_parameters.get("knn_neighbors", 5)
        â”‚   â”‚       imputer = KNNImputer(n_neighbors=n_neighbors)
        â”‚   â”‚       imputed_data = imputer.fit_transform(cleaned_data)
        â”‚   â”‚   # ... other techniques (MICE, drop_rows, etc.)
        â”‚   â”‚
        â”‚   â””â”€> Log to MLflow
        â”‚
        â”œâ”€> 4. EXECUTE ENCODE_FEATURES TECHNIQUE
        â”‚   â”œâ”€> Execute based on selection:
        â”‚   â”‚   if encode_technique == "target_encoding":
        â”‚   â”‚       smoothing = technique_parameters.get("target_smoothing", 1.0)
        â”‚   â”‚       encoder = TargetEncoder(smoothing=smoothing)
        â”‚   â”‚       encoded_data = encoder.fit_transform(
        â”‚   â”‚           imputed_data, target_column)
        â”‚   â”‚   elif encode_technique == "one_hot":
        â”‚   â”‚       encoded_data = pd.get_dummies(imputed_data)
        â”‚   â”‚   # ... other techniques (frequency, hash, etc.)
        â”‚   â”‚
        â”‚   â””â”€> Log to MLflow
        â”‚
        â”œâ”€> 5. EXECUTE SCALE_FEATURES TECHNIQUE
        â”‚   â”œâ”€> Execute based on selection:
        â”‚   â”‚   if scale_technique == "none":
        â”‚   â”‚       scaled_data = encoded_data.copy()
        â”‚   â”‚       scaler = None
        â”‚   â”‚   elif scale_technique == "standard_scaler":
        â”‚   â”‚       scaler = StandardScaler()
        â”‚   â”‚       scaled_data = scaler.fit_transform(encoded_data)
        â”‚   â”‚   elif scale_technique == "minmax_scaler":
        â”‚   â”‚       scaler = MinMaxScaler()
        â”‚   â”‚       scaled_data = scaler.fit_transform(encoded_data)
        â”‚   â”‚   # ... other techniques (robust, maxabs, etc.)
        â”‚   â”‚
        â”‚   â””â”€> Log to MLflow with algorithm context:
        â”‚       mlflow.log_params({
        â”‚           "scale_technique": scale_technique,
        â”‚           "algorithm_category": state["algorithm_category"],
        â”‚           "scale_skipped_reason": "Tree models scale-invariant"
        â”‚               if scale_technique == "none" else None
        â”‚       })
        â”‚
        â””â”€> 6. UPDATE STATE
            â””â”€> state.update({
                    "cleaned_data": scaled_data,
                    "scaler": scaler,
                    "feature_metadata": {
                        "algorithm_category": state["algorithm_category"],
                        "techniques_used": {
                            "clean_data": clean_data_technique,
                            "handle_missing": handle_missing_technique,
                            "encode_features": encode_technique,
                            "scale_features": scale_technique
                        },
                        "technique_parameters": technique_parameters,
                        "outliers_removed": outliers_removed,
                        "missing_imputed": len(missing_values),
                        "encoding_mappings": encoder.mapping_ if encoder else None
                    },
                    "status": "preprocessing_completed"
                })
```

---

## Summary

This document provides three levels of flow diagrams:

### Level 1 (1LD): High-Level Overview
- **Audience**: Executives, Product Managers
- **Focus**: What the system does
- **Detail**: Conceptual phases and major decisions

### Level 2 (2LD): Detailed Component Interactions
- **Audience**: Architects, Senior Engineers
- **Focus**: How components interact
- **Detail**: Component boundaries, data flow, integration points

### Level 3 (3LD): Implementation Details
- **Audience**: Developers, QA Engineers
- **Focus**: How to implement each component
- **Detail**: Code-level logic, algorithms, API calls, error handling

Each level provides progressively more detail:
- **1LD**: 5-7 major blocks
- **2LD**: 20-30 components with interactions
- **3LD**: Line-by-line implementation logic

Use these diagrams for:
- **System Design Reviews**: Present 1LD to stakeholders
- **Architecture Planning**: Use 2LD for component design
- **Development**: Follow 3LD for implementation
- **Documentation**: Complete reference at all levels
- **Onboarding**: Progressive learning from 1LD â†’ 2LD â†’ 3LD

---

**End of Flow Diagrams Document**
