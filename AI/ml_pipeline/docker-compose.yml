version: '3.8'

services:
  # PostgreSQL Database with pgvector - Prompt Storage + Vector Embeddings
  postgres:
    image: pgvector/pgvector:pg14
    container_name: ml-pipeline-postgres
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-ml_pipeline}
      POSTGRES_USER: ${POSTGRES_USER:-ml_pipeline_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/schema:/docker-entrypoint-initdb.d:ro
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-ml_pipeline_user} -d ${POSTGRES_DB:-ml_pipeline}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - ml-pipeline-network
    restart: unless-stopped

  # MLflow Tracking Server
  mlflow:
    build:
      context: .
      dockerfile: Dockerfile.mlflow
    container_name: mlflow-server
    command: >
      mlflow server
      --backend-store-uri postgresql://${POSTGRES_USER:-ml_pipeline_user}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/${POSTGRES_DB:-ml_pipeline}
      --default-artifact-root file:///mlflow/artifacts
      --host 0.0.0.0
      --port 5000
    volumes:
      - mlflow_artifacts:/mlflow/artifacts
    ports:
      - "${MLFLOW_PORT:-5000}:5000"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # FastAPI Backend
  backend:
    build:
      context: ..
      dockerfile: ml_pipeline/Dockerfile.backend
    container_name: ml-pipeline-backend
    ports:
      - "${API_PORT:-8000}:8000"
    volumes:
      - ./data:/app/data
      - ./outputs:/app/outputs
      - ./mlruns:/app/mlruns
      - ./config:/app/config
      - .env:/app/.env
      - ~/.aws:/root/.aws:ro  # Mount AWS credentials (read-only)
      - mlflow_artifacts:/mlflow/artifacts  # Share MLflow artifact storage
    environment:
      # PostgreSQL
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=${POSTGRES_DB:-ml_pipeline}
      - POSTGRES_USER=${POSTGRES_USER:-ml_pipeline_user}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-changeme}
      # AWS Bedrock (credentials from mounted ~/.aws directory)
      - BEDROCK_MODEL_ID=${BEDROCK_MODEL_ID:-us.anthropic.claude-sonnet-4-5-20250929-v1:0}
      - BEDROCK_FALLBACK_MODEL_ID=${BEDROCK_FALLBACK_MODEL_ID:-anthropic.claude-3-7-sonnet-20250219-v1:0}
      - AWS_REGION=${AWS_REGION:-us-east-1}
      - AWS_PROFILE=${AWS_PROFILE:-default}
      # MLflow
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      # Application
      - PYTHONUNBUFFERED=1
    depends_on:
      postgres:
        condition: service_healthy
      mlflow:
        condition: service_healthy
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # Vue.js 3 Frontend (ChatGPT-like UI)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: ml-pipeline-frontend
    ports:
      - "3000:80"
    environment:
      - VITE_API_BASE_URL=http://localhost:8000
      - VITE_MLFLOW_URL=http://localhost:5000
      - VITE_WS_URL=ws://localhost:8000
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - ml-pipeline-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

networks:
  ml-pipeline-network:
    driver: bridge
    name: ml-pipeline-network

volumes:
  postgres_data:
    name: ml-pipeline-postgres-data
  mlflow_artifacts:
    name: ml-pipeline-mlflow-artifacts
  mlruns:
  data:
  outputs:
