# ===================================
# Enhanced ML Pipeline Configuration
# ===================================
# Copy this file to .env and fill in your values

# ===================================
# AWS Configuration
# ===================================
AWS_REGION=us-east-1
AWS_PROFILE=default
# Optional: Explicitly set credentials (not recommended, use AWS CLI configure instead)
# AWS_ACCESS_KEY_ID=your_access_key
# AWS_SECRET_ACCESS_KEY=your_secret_key

# ===================================
# AWS Bedrock Configuration
# ===================================
BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
BEDROCK_TEMPERATURE=0.0
BEDROCK_MAX_TOKENS=4096
BEDROCK_ENABLE_AGENTS=true

# Alternative Bedrock Models:
# anthropic.claude-3-haiku-20240307-v1:0  (Faster, cheaper)
# anthropic.claude-3-opus-20240229-v1:0   (Most capable)

# ===================================
# MLflow Configuration
# ===================================
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=ml_pipeline_experiment
MLFLOW_ENABLE_LOGGING=true

# For remote MLflow server:
# MLFLOW_TRACKING_URI=https://your-mlflow-server.com
# MLFLOW_TRACKING_USERNAME=username
# MLFLOW_TRACKING_PASSWORD=password

# MLflow Model Registry
MLFLOW_REGISTRY_URI=
MLFLOW_DEFAULT_ARTIFACT_ROOT=

# ===================================
# Pipeline Configuration
# ===================================
DEFAULT_DATA_PATH=data/raw/train.csv
DEFAULT_TARGET_COLUMN=target
DEFAULT_TEST_SIZE=0.2
DEFAULT_RANDOM_STATE=42

# Output Directories
OUTPUT_DIR=outputs
MODELS_DIR=outputs/models
METRICS_DIR=outputs/metrics
PLOTS_DIR=outputs/plots
REPORTS_DIR=outputs/reports
LOGS_DIR=outputs/logs

# ===================================
# Hyperparameter Tuning Configuration
# ===================================
ENABLE_HYPERPARAMETER_TUNING=true
TUNING_METHOD=grid_search  # Options: grid_search, random_search
TUNING_CV_FOLDS=5
TUNING_N_JOBS=-1  # Use all CPU cores
TUNING_VERBOSE=1

# ===================================
# Monitoring Configuration
# ===================================
ENABLE_DRIFT_DETECTION=true
DRIFT_THRESHOLD_P_VALUE=0.05
DRIFT_THRESHOLD_PSI=0.2

ENABLE_PERFORMANCE_MONITORING=true
PERFORMANCE_DEGRADATION_THRESHOLD=0.05  # 5% drop triggers alert

# ===================================
# Retraining Configuration
# ===================================
ENABLE_AUTO_RETRAINING=true
RETRAINING_SCHEDULE_CRON=0 2 * * *  # Daily at 2 AM
RETRAINING_PERFORMANCE_THRESHOLD=0.1  # 10% drop triggers retraining
RETRAINING_DRIFT_THRESHOLD=3  # Number of drifted features

# ===================================
# Algorithm Selection
# ===================================
# Comma-separated list of algorithms to consider
CLASSIFICATION_ALGORITHMS=logistic_regression,random_forest,gradient_boosting,svm,knn
REGRESSION_ALGORITHMS=linear_regression,ridge,lasso,random_forest,gradient_boosting

# Enable optional advanced algorithms (requires additional packages)
ENABLE_XGBOOST=false
ENABLE_LIGHTGBM=false
ENABLE_CATBOOST=false

# ===================================
# Agent Configuration
# ===================================
# Agent retry settings
AGENT_MAX_RETRIES=3
AGENT_RETRY_DELAY=2  # seconds
AGENT_TIMEOUT=60  # seconds

# Enable/disable specific agents
ENABLE_ALGORITHM_SELECTION_AGENT=true
ENABLE_MODEL_SELECTION_AGENT=true
ENABLE_RETRAINING_DECISION_AGENT=true

# ===================================
# Logging Configuration
# ===================================
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FORMAT=json  # Options: json, text
LOG_FILE=logs/pipeline.log
ENABLE_CONSOLE_LOGGING=true
ENABLE_FILE_LOGGING=true

# ===================================
# Performance and Resource Configuration
# ===================================
N_JOBS=-1  # Number of parallel jobs (-1 = use all cores)
MEMORY_LIMIT_GB=8  # Maximum memory usage in GB
BATCH_SIZE=1000  # Batch size for large dataset processing

# ===================================
# Validation Configuration
# ===================================
ENABLE_DATA_VALIDATION=true
MIN_SAMPLES=100  # Minimum number of samples required
MAX_MISSING_RATIO=0.3  # Maximum ratio of missing values per feature
MIN_CLASS_SAMPLES=10  # Minimum samples per class for classification

# ===================================
# Development/Debug Settings
# ===================================
DEBUG_MODE=false
ENABLE_PROFILING=false
SAVE_INTERMEDIATE_RESULTS=false

# ===================================
# Notification Configuration (Optional)
# ===================================
ENABLE_NOTIFICATIONS=false
NOTIFICATION_EMAIL=
NOTIFICATION_SLACK_WEBHOOK=

# ===================================
# Database Configuration (Optional)
# ===================================
# For storing pipeline metadata
DATABASE_URL=sqlite:///pipeline.db
# For PostgreSQL:
# DATABASE_URL=postgresql://user:password@localhost:5432/ml_pipeline

# ===================================
# Cloud Storage Configuration (Optional)
# ===================================
# S3 for artifacts
S3_BUCKET_NAME=
S3_ARTIFACTS_PREFIX=ml-pipeline-artifacts

# Azure Blob Storage
AZURE_STORAGE_ACCOUNT=
AZURE_STORAGE_CONTAINER=

# ===================================
# Security Configuration
# ===================================
ENABLE_ENCRYPTION=false
ENCRYPTION_KEY=
